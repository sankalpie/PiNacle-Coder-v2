{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://datasets-server.huggingface.co/parquet?dataset=code_contests\"\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "data = query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|██████████| 4.52k/4.52k [00:00<00:00, 2.26MB/s]\n",
      "Downloading readme: 100%|██████████| 13.0k/13.0k [00:00<00:00, 5.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_contests/default (download: 7.10 GiB, generated: 18.07 GiB, post-processed: Unknown size, total: 25.17 GiB) to C:/Users/Sankalpie/.cache/huggingface/datasets/deepmind___parquet/deepmind--code_contests-c83aff5e5b92def4/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 180M/180M [00:20<00:00, 8.65MB/s]\n",
      "Downloading data: 100%|██████████| 209M/209M [00:23<00:00, 8.84MB/s]\n",
      "Downloading data: 100%|██████████| 227M/227M [00:34<00:00, 6.66MB/s]\n",
      "Downloading data: 100%|██████████| 181M/181M [00:19<00:00, 9.09MB/s]\n",
      "Downloading data: 100%|██████████| 195M/195M [00:21<00:00, 9.13MB/s]\n",
      "Downloading data: 100%|██████████| 174M/174M [00:19<00:00, 9.04MB/s]\n",
      "Downloading data: 100%|██████████| 186M/186M [00:20<00:00, 9.04MB/s]\n",
      "Downloading data: 100%|██████████| 172M/172M [00:19<00:00, 8.98MB/s]\n",
      "Downloading data: 100%|██████████| 200M/200M [00:21<00:00, 9.16MB/s]\n",
      "Downloading data: 100%|██████████| 205M/205M [00:22<00:00, 9.10MB/s]\n",
      "Downloading data: 100%|██████████| 178M/178M [00:19<00:00, 9.00MB/s]\n",
      "Downloading data: 100%|██████████| 164M/164M [00:18<00:00, 9.04MB/s]\n",
      "Downloading data: 100%|██████████| 200M/200M [00:21<00:00, 9.19MB/s]\n",
      "Downloading data: 100%|██████████| 197M/197M [00:21<00:00, 9.06MB/s]\n",
      "Downloading data: 100%|██████████| 211M/211M [00:24<00:00, 8.59MB/s]\n",
      "Downloading data: 100%|██████████| 179M/179M [00:19<00:00, 9.10MB/s]\n",
      "Downloading data: 100%|██████████| 202M/202M [00:22<00:00, 8.91MB/s]\n",
      "Downloading data: 100%|██████████| 169M/169M [00:19<00:00, 8.77MB/s]\n",
      "Downloading data: 100%|██████████| 185M/185M [00:21<00:00, 8.57MB/s]\n",
      "Downloading data: 100%|██████████| 191M/191M [00:22<00:00, 8.52MB/s]\n",
      "Downloading data: 100%|██████████| 211M/211M [00:24<00:00, 8.80MB/s]\n",
      "Downloading data: 100%|██████████| 181M/181M [00:19<00:00, 9.10MB/s]\n",
      "Downloading data: 100%|██████████| 194M/194M [00:21<00:00, 9.16MB/s]\n",
      "Downloading data: 100%|██████████| 176M/176M [00:19<00:00, 9.03MB/s]\n",
      "Downloading data: 100%|██████████| 181M/181M [00:20<00:00, 8.91MB/s]\n",
      "Downloading data: 100%|██████████| 206M/206M [00:25<00:00, 8.21MB/s]\n",
      "Downloading data: 100%|██████████| 189M/189M [00:20<00:00, 9.08MB/s]\n",
      "Downloading data: 100%|██████████| 217M/217M [00:31<00:00, 6.83MB/s]\n",
      "Downloading data: 100%|██████████| 179M/179M [00:25<00:00, 7.15MB/s]\n",
      "Downloading data: 100%|██████████| 198M/198M [00:31<00:00, 6.37MB/s]\n",
      "Downloading data: 100%|██████████| 223M/223M [00:32<00:00, 6.88MB/s]\n",
      "Downloading data: 100%|██████████| 181M/181M [00:27<00:00, 6.70MB/s]\n",
      "Downloading data: 100%|██████████| 186M/186M [00:20<00:00, 9.14MB/s]\n",
      "Downloading data: 100%|██████████| 204M/204M [00:22<00:00, 9.07MB/s]\n",
      "Downloading data: 100%|██████████| 188M/188M [00:31<00:00, 5.87MB/s]\n",
      "Downloading data: 100%|██████████| 151M/151M [00:17<00:00, 8.78MB/s]\n",
      "Downloading data: 100%|██████████| 204M/204M [00:30<00:00, 6.78MB/s]\n",
      "Downloading data: 100%|██████████| 231M/231M [00:25<00:00, 9.11MB/s]\n",
      "Downloading data: 100%|██████████| 204M/204M [00:22<00:00, 9.03MB/s]\n",
      "Downloading data: 100%|██████████| 63.1M/63.1M [00:07<00:00, 8.41MB/s]]\n",
      "Downloading data: 100%|██████████| 51.8M/51.8M [00:06<00:00, 8.15MB/s] \n",
      "Downloading data files: 100%|██████████| 3/3 [17:06<00:00, 342.03s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 11.06it/s]\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/Sankalpie/.cache/huggingface/datasets/deepmind___parquet/deepmind--code_contests-c83aff5e5b92def4/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('deepmind/code_contests', split='train')\n",
    "df=pd.DataFrame(dataset)\n",
    "df.to_csv(\"questions_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m./questions_dataset.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:235\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    234\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m    236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:790\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"./questions_dataset.csv\",low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:630: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# df = dd.read_csv('./questions_dataset.csv')\n",
    "\n",
    "specific_columns_dtypes = {'untranslated_description': 'object',\n",
    "                           'input_file':'object',\n",
    "                           'output_file':'object',}\n",
    "df=dd.read_csv(\"./questions_dataset.csv\",dtype=specific_columns_dtypes,error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>public_tests</th>\n",
       "      <th>private_tests</th>\n",
       "      <th>generated_tests</th>\n",
       "      <th>source</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>solutions</th>\n",
       "      <th>incorrect_solutions</th>\n",
       "      <th>cf_contest_id</th>\n",
       "      <th>cf_index</th>\n",
       "      <th>cf_points</th>\n",
       "      <th>cf_rating</th>\n",
       "      <th>cf_tags</th>\n",
       "      <th>is_description_translated</th>\n",
       "      <th>untranslated_description</th>\n",
       "      <th>time_limit</th>\n",
       "      <th>memory_limit_bytes</th>\n",
       "      <th>input_file</th>\n",
       "      <th>output_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brcktsrm</td>\n",
       "      <td>Problem description.\\nVipul is a hardworking s...</td>\n",
       "      <td>{'input': ['3\\n((()))\\n(())()\\n()(()'], 'outpu...</td>\n",
       "      <td>{'input': [], 'output': []}</td>\n",
       "      <td>{'input': ['3\\n((()))\\n(())()\\n()())', '3\\n(((...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'language': [1, 1, 1], 'solution': [\"for _ in...</td>\n",
       "      <td>{'language': [], 'solution': []}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comm3</td>\n",
       "      <td>The Chef likes to stay in touch with his staff...</td>\n",
       "      <td>{'input': ['3\\n1\\n0 1\\n0 0\\n1 0\\n2\\n0 1\\n0 0\\n...</td>\n",
       "      <td>{'input': [], 'output': []}</td>\n",
       "      <td>{'input': ['3\\n1\\n0 1\\n0 -1\\n1 0\\n2\\n0 1\\n0 0\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>{'language': [], 'solution': []}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gcd2</td>\n",
       "      <td>Frank explained its friend Felman the algorith...</td>\n",
       "      <td>{'input': ['2\\n2 6\\n10 11'], 'output': ['2\\n1\\...</td>\n",
       "      <td>{'input': [], 'output': []}</td>\n",
       "      <td>{'input': ['2\\n3 6\\n10 11', '2\\n1 3\\n10 11', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>{'language': [], 'solution': []}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luckybal</td>\n",
       "      <td>A Little Elephant from the Zoo of Lviv likes l...</td>\n",
       "      <td>{'input': ['4\\n47\\n74\\n477\\n4747477'], 'output...</td>\n",
       "      <td>{'input': [], 'output': []}</td>\n",
       "      <td>{'input': ['4\\n47\\n47\\n477\\n4747477', '4\\n7\\n4...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'language': [1, 1, 1, 1, 1], 'solution': [\"n ...</td>\n",
       "      <td>{'language': [], 'solution': []}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prpaln</td>\n",
       "      <td>Given a string s. Can you make it a palindrome...</td>\n",
       "      <td>{'input': ['4\\naaa\\nabc\\nabdbca\\nabba'], 'outp...</td>\n",
       "      <td>{'input': [], 'output': []}</td>\n",
       "      <td>{'input': ['4\\naaa\\nabc\\nabdbca\\nabca', '4\\naa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>{'language': [], 'solution': []}</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                        description  \\\n",
       "0  brcktsrm  Problem description.\\nVipul is a hardworking s...   \n",
       "1     comm3  The Chef likes to stay in touch with his staff...   \n",
       "2      gcd2  Frank explained its friend Felman the algorith...   \n",
       "3  luckybal  A Little Elephant from the Zoo of Lviv likes l...   \n",
       "4    prpaln  Given a string s. Can you make it a palindrome...   \n",
       "\n",
       "                                        public_tests  \\\n",
       "0  {'input': ['3\\n((()))\\n(())()\\n()(()'], 'outpu...   \n",
       "1  {'input': ['3\\n1\\n0 1\\n0 0\\n1 0\\n2\\n0 1\\n0 0\\n...   \n",
       "2  {'input': ['2\\n2 6\\n10 11'], 'output': ['2\\n1\\...   \n",
       "3  {'input': ['4\\n47\\n74\\n477\\n4747477'], 'output...   \n",
       "4  {'input': ['4\\naaa\\nabc\\nabdbca\\nabba'], 'outp...   \n",
       "\n",
       "                 private_tests  \\\n",
       "0  {'input': [], 'output': []}   \n",
       "1  {'input': [], 'output': []}   \n",
       "2  {'input': [], 'output': []}   \n",
       "3  {'input': [], 'output': []}   \n",
       "4  {'input': [], 'output': []}   \n",
       "\n",
       "                                     generated_tests  source  difficulty  \\\n",
       "0  {'input': ['3\\n((()))\\n(())()\\n()())', '3\\n(((...       1           6   \n",
       "1  {'input': ['3\\n1\\n0 1\\n0 -1\\n1 0\\n2\\n0 1\\n0 0\\...       1           1   \n",
       "2  {'input': ['2\\n3 6\\n10 11', '2\\n1 3\\n10 11', '...       1           2   \n",
       "3  {'input': ['4\\n47\\n47\\n477\\n4747477', '4\\n7\\n4...       1           2   \n",
       "4  {'input': ['4\\naaa\\nabc\\nabdbca\\nabca', '4\\naa...       1           2   \n",
       "\n",
       "                                           solutions  \\\n",
       "0  {'language': [1, 1, 1], 'solution': [\"for _ in...   \n",
       "1  {'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "2  {'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "3  {'language': [1, 1, 1, 1, 1], 'solution': [\"n ...   \n",
       "4  {'language': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                incorrect_solutions  cf_contest_id cf_index  cf_points  \\\n",
       "0  {'language': [], 'solution': []}              0      NaN        0.0   \n",
       "1  {'language': [], 'solution': []}              0      NaN        0.0   \n",
       "2  {'language': [], 'solution': []}              0      NaN        0.0   \n",
       "3  {'language': [], 'solution': []}              0      NaN        0.0   \n",
       "4  {'language': [], 'solution': []}              0      NaN        0.0   \n",
       "\n",
       "   cf_rating cf_tags  is_description_translated untranslated_description  \\\n",
       "0          0      []                      False                      NaN   \n",
       "1          0      []                      False                      NaN   \n",
       "2          0      []                      False                      NaN   \n",
       "3          0      []                      False                      NaN   \n",
       "4          0      []                      False                      NaN   \n",
       "\n",
       "  time_limit  memory_limit_bytes input_file output_file  \n",
       "0        NaN                   0        NaN         NaN  \n",
       "1        NaN                   0        NaN         NaN  \n",
       "2        NaN                   0        NaN         NaN  \n",
       "3        NaN                   0        NaN         NaN  \n",
       "4        NaN                   0        NaN         NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39;49mcompute())\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(length)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[39m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[39m=\u001b[39m pandas_read_text(\n\u001b[0;32m    143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader,\n\u001b[0;32m    144\u001b[0m     block,\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheader,\n\u001b[0;32m    146\u001b[0m     rest_kwargs,\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtypes,\n\u001b[0;32m    148\u001b[0m     columns,\n\u001b[0;32m    149\u001b[0m     write_header,\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menforce,\n\u001b[0;32m    151\u001b[0m     path_info,\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns]\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\io\\csv.py:195\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    193\u001b[0m bio\u001b[39m.\u001b[39mwrite(b)\n\u001b[0;32m    194\u001b[0m bio\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m df \u001b[39m=\u001b[39m reader(bio, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m dtypes:\n\u001b[0;32m    197\u001b[0m     coerce_dtypes(df, dtypes)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "length = len(df.compute())\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'description', 'public_tests', 'private_tests',\n",
       "       'generated_tests', 'source', 'difficulty', 'solutions',\n",
       "       'incorrect_solutions', 'cf_contest_id', 'cf_index', 'cf_points',\n",
       "       'cf_rating', 'cf_tags', 'is_description_translated',\n",
       "       'untranslated_description', 'time_limit', 'memory_limit_bytes',\n",
       "       'input_file', 'output_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "cf_contest_id               int64\n",
       "untranslated_description      ...\n",
       "dtype: int64\n",
       "Dask Name: dataframe-sum-agg, 4 graph layers"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sankalpie\\anaconda3\\envs\\my_tensorflow_gpu_env\\lib\\site-packages\\dask\\dataframe\\core.py:8069: UserWarning: Insufficient elements for `head`. 1000 elements requested, only 53 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_new = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"./questions_dataset_small.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_new=df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_new.to_csv(\"./questions_dataset_very_small.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_file = pq.ParquetFile('./dataset/paraquet_files/parquet-train-00000-of-00039.parquet')\n",
    "table = parquet_file.read()\n",
    "\n",
    "# Convert the table to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('./questions_dataset_from_paraquet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./questions_dataset_from_paraquet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tensorflow_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
